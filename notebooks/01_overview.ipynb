{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototypes of real-time analysis system for neural data\n",
    "\n",
    "_Author_: Conor McGrory (conor.mcgrory@stonybrook.edu), rotation student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This repository contains the code and notes from my rotation project in the Park Lab from September to December 2020. The goal of this project was to develop multiple prototypes of a real-time analysis system for neural electrophysiological data, using different platforms, in order to better understand both the technical challenges that need to be overcome in order to build such as system and the tools best suited for the problem. \n",
    "\n",
    "### How to use the notebooks\n",
    "\n",
    "The notebooks in this folder are meant to be read/run in order, and contain all of the code necessary for fetching required input data and analyzing experimental results, as well as detailed instructions for running the experiments themselves. \n",
    "\n",
    "### Notebook index\n",
    "\n",
    "The notebooks in this directory are:\n",
    "\n",
    "- [01_overview.ipynb](./01_overview.ipynb): The notebook you are currently reading. Contains a broad overview of the project and explains its results.\n",
    "- [02_raw_data.ipynb](./02_raw_data.ipynb): Contains code for downloading and examining the data used for this project. \n",
    "- [03_test_signal.ipynb](./03_test_signal.ipynb): Contains code for compiling raw data into signal that is used to test the prototypes, and examining signal.\n",
    "- [04_lms_filter.ipynb](./04_lms_filter.ipynb): Explores the properties of least-mean-squares (LMS) filtering, an adaptive filtering method used for the prototypes\n",
    "- [05_results_python.ipynb](./05_results_python.ipynb): Contains instructions for running latency experiment with Python prototype, along with code for analyzing results\n",
    "- [06_results_julia.ipynb](./06_results_julia.ipynb): Contains instructions for running latency experiment with Julia prototype, along with code for analyzing results\n",
    "- [07_results_rust.ipynb](./07_results_rust.ipynb): Contains instructions for running latency experiment with Rust prototype, along with code for analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Real-time analysis of neural data\n",
    "\n",
    "The broad research objective that motivates this project is the development of a real-time analysis system for electrophysiological data. Being able to process neural data in real-time during the course of an experiment would enable us to perform a whole new class of experimental manipulations, including closed-loop experiments and custom perturbations of neural circuits.\n",
    "\n",
    "![filter setup](img/filter_setup.png)\n",
    "\n",
    "Our proposed setup uses two computers. The first, called the \"probe\" machine, interfaces directly with the probe used to collect the data (in our case, this will most likely be the [Neuropixel](https://www.neuropixels.org/)). At each time step, it reads data from the probe and sends it to the second computer, called the \"processor,\" which uses the incoming data to update an adaptive filter or some other type of online data analysis method. In the case of a closed-loop experiment, the \"processor\" machine will send data back to the \"probe\" machine that will change some element of the experiment in response to a particular state of the filter.\n",
    "\n",
    "One of the most significant technical challenges that needs to be overcome in order to make this system possible is the problem of _latency_. Modern probes, such as the Neuropixel, record data at very high sample rates, which in theory allows us to perform closed-loop experiments with a high level of temporal precision. However, the limiting factor currently preventing this is the time it takes for the analysis method to run. The broad aim of this project was to build simple prototypes of the real-time analysis system on a number of different platforms in order to understand how to minimize latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The main objectives for this project were:\n",
    "1. Build simple prototypes of the real-time analysis system using three different languages: Python, Julia, and Rust\n",
    "2. Measure the latency properties of all three prototypes and compare them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype setup\n",
    "\n",
    "![prototype](./img/prototype.png)\n",
    "\n",
    "The basic structure of the prototype is set up to mimic the conditions of an actual experiment as well as we can _in silico_. The probe and processor machines are simulated using two processes, which can either be run on two separate machines connected via Ethernet, or on a single machine using the local link hardware. In this setup, the processor machine functions as a server process, and the probe as a client. The processor, when run, waits for the probe to connect using a TCP/IP socket on a prespecified port. Once the probe is connected, it starts sending the test data, which consists of vectors of spike counts, across the socket, to the processor, which returns a \"filter prediction\" for each data point. The test data (see below) is loaded from an HDF5 file before the data transmission begins, in order to ensure that all measured latency is the result of the prototype itself.\n",
    "\n",
    "When transmitting the data, the probe sends the spike count vectors one at a time, waiting until it recieves a response from the processor before it sends another one. While this \"blocking\" approach would clearly be suboptimal if used in a real experiment, it allows us to measure the round-trip latency -- the elapsed time between sending a spike vector and recieving its filter prediction -- in a straightforward and accurate way. In future prototypes, this approach will be replaced by an approach using two nonblocking sockets: one for sending spike data from probe to processor, and the other for feedback data from processor to probe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "\n",
    "![raw data](./img/raw_data.png)\n",
    "\n",
    "The neural data we use to test our prototypes is a publicly available dataset from [Steinmetz et al., 2019](https://www.nature.com/articles/s41586-019-1787-x). This data contains 698 neurons, and uses a bin size of 10 ms. Individual trials are 2.5 sec long, and contain 250 data points. The notebook [02_raw_data.ipynb](./02_raw_data.ipynb) contains the code used to download this data from the server and examine it locally.\n",
    "\n",
    "Because the trials are relatively short, we concatenate all of the trials for a particular session into a longer signal, called the \"test signal\", for use with the prototypes. This is explained further in [03_test_signal.ipynb](./03_test_signal.ipynb), which contains the code used to create this signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-mean-squares filter\n",
    "\n",
    "For the processor component of the prototypes, we used a simple adaptive filter called a [least-mean-squares (LMS) filter](https://en.wikipedia.org/wiki/Least_mean_squares_filter). Because this filter is linear, it isn't likely to be useful for any actual neural data analysis, but its ease of implementation and relatively low computational cost made it a good algorithm to use for measuring latency in different prototypes. For an exploration of LMS filtering and its application to our test data, see [04_lms_filter.ipynb](./04_lms_filter.ipynb).\n",
    "\n",
    "![lms](./img/lms_synthetic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex filters\n",
    "\n",
    "The initial goal of this project was to implement a more complex filter, like a Kalman filter, in the prototypes, to see what the latency of our system would be when using the kind of filter we would actually need to extract useful information from neural data. However, because I ended up spending more time than I expected decreasing the latency of the prototypes using the LMS filter, this will need to be left for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of programming languages\n",
    "\n",
    "The reason I ended up creating prototypes of the real-time analysis system in three different programming languages was because, due to the nature of the task, there was no obvious best choice for a language to use. Offline data analysis tasks are usually best accomplished using a high-level language such as Python or MATLAB, which allows researchers to quickly and easily code up different analysis pipelines without worrying about low-level issues like memory management. The fact that these languages are significantly slower compared to more low-level languages like C or Fortran isn't an issue, because the time constraints for these tasks are relatively loose. For our task, however, speed is extremely important -- in order for our method to be experimentally useful, it needs to be able to analyze data as fast as it is collected. This puts us in a more tough position then: we need the speed that comes with low-level languages, but we also need to be able to quickly and reliably implement somewhat complex filtering operations to run on the data.\n",
    "\n",
    "The first language I used to build a prototype of the real-time analysis system was [Julia](https://julialang.org/). This high-level language was attractive for a number of reasons, the chief among them being that because of its type system and just-in-time (JIT) compiler, it can perform certain benchmark computational tasks almost as fast as C, while being much easier to program in. The network programming functionality ended up being difficult to work with, and caused some early latency problems that were only resolved recently.\n",
    "\n",
    "In order to compare Julia against a purely high-level language, I wrote the second prototype in [Python](https://www.python.org/). This prototype was easy to build, and because of the straightforward interface for network programming, and the use of NumPy for the filtering operation, it worked decently well. \n",
    "\n",
    "Finally, I wanted to try using a low-level language, to see if this could decrease the latency even more. While C is a much more established language, I ended up choosing [Rust](https://www.rust-lang.org/) because of its memory management features and a third-party linear algebra library called [ndarray](https://docs.rs/ndarray/0.14.0/ndarray/) that supports a NumPy-style interface for matrix computations. \n",
    "\n",
    "Below is a table showing the strengths and weaknesses of the three languages used for the prototypes:\n",
    "\n",
    "|                             | Python          | Julia     | Rust           |\n",
    "| --------------------------- | --------------- | --------- | -------------- |\n",
    "| Learning curve              | Very easy       | Very easy | More difficult |\n",
    "| Speed                       | Relatively slow | Faster    | Fastest        |\n",
    "| Numerical computing support | Very good       | Very good | Some           |\n",
    "| Network support             | Good            | Good      | Good           |\n",
    "| Application support         | Good            | Not good  | Good           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echo filter latencies\n",
    "\n",
    "![echo latencies](./img/echo_latencies.png)\n",
    "\n",
    "As this plot clearly shows, the echo filter latency distributions of the Python and Rust prototypes are very similar, while the Julia prototype is slightly slower. The median latency for the Python prototype is 45 $\\mu s$, the median latency for the Rust prototype is 37 $\\mu s$, and the median latency for the Julia prototype is 110 $\\mu s$. In some ways, this is what we would expect -- the echo filter doesn't process the data at all, so the only sources of latency here are the socket overhead, which, being a kernel call, should be similar for all prototypes, and the cost associated with encoding data into bytes to send it across the socket and decoding it on the other end, which should be minimal. However, it is unclear why the Julia prototype is slower. One likely explanation would be that it has something to do with either how the data is encoded before sending it over the socket, or the Julia network API itself, which seems to be less direct to the kernel than the Python or Rust socket APIs. This is an important result because it establishes that any latency differences we see with the LMS filter are the caused by the code that implements the filter and nothing else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMS filter latencies\n",
    "\n",
    "![lms latencies](./img/lms_latencies.png)\n",
    "\n",
    "In this plot of the LMS filter latency distributions of the Python and Rust prototypes, it is clear that the Python prototype is faster. This is also reflected in the median values (Python: 8717 $\\mu s$, Rust: 11174 $\\mu s$). Because the echo filter latencies of these two prototypes were extremely close, this difference is almost certainly due to the LMS filter code. \n",
    "\n",
    "There are two explanations I can think of for this. The first is that the [ndarray](https://docs.rs/ndarray/0.14.0/ndarray/) package that the Rust prototype uses to implement the LMS filter is slower than NumPy -- this could be caused by differences in how the two packages call out to the LAPACK or BLAS libraries for matrix computations. The other possiblity is that the way the LMS filter code in the Rust prototype is written is inefficient in terms of memory allocation. This is definitely possible, considering that I had no prior experience programming in Rust before working on this project, and still have some issues working with its memory management system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
